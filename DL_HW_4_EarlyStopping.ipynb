{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL HW#4-EarlyStopping",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8AbGfbRPYwK",
        "colab_type": "code",
        "outputId": "a920a848-780f-491c-f699-c1bbaad0f999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2eLc_uP8Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WMO-YSRTilx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1] * train_images.shape[2]).astype('float32') / 255\n",
        "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1] * test_images.shape[2]).astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bV9sASxTnaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = np_utils.to_categorical(train_labels, 10)\n",
        "test_labels = np_utils.to_categorical(test_labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX993TkoZdcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit에서 callback을 쓰지않고 각 epoch 에서 loss를 얻어내기 위해 케라스가 아닌 텐서플로우로 구현하였습니다.\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "k = tf.matmul(x, W) + b\n",
        "y = tf.nn.softmax(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EneticmoaQpR",
        "colab_type": "code",
        "outputId": "44b31bbb-8342-4b56-e186-bb75e0aaaff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "y_ = tf.placeholder(tf.float32, [None, 10])                                                                               \n",
        "# learning rate가 0.1일때 loss가 더 느리게 감소하지만, 훨씬 loss가 적게 나옵니다.\n",
        "# 다만 여기서는 같은 epoch를 기준으로 비교해야될거같아서 일부러 0.2로 하였습니다.\n",
        "# 0.1로 하게되면 300번을 넘겨도 학습이 끝나지않으므로, early stopping을 적용한 상태에서는 다른 모델들과 같은 epoch로 비교할 수가 없기 때문입니다.\n",
        "# 참고로 learning_rate를 0.1로 하였을 경우, accuracy는 약 0.88 정도가 나왔습니다.\n",
        "learning_rate = 0.2  \n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = k, labels = y_))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 07:26:25.559775 139793404135296 deprecation.py:323] From <ipython-input-6-46c63e4834d5>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZg9jDGgdiG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping():\n",
        "  def __init__(self, patience=0, verbose=0):\n",
        "    self._step = 0\n",
        "    self._loss = float('inf')\n",
        "    self.patience  = patience\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def validate(self, loss):\n",
        "    if self._loss < loss:             # 현재의 loss가 기존 loss보다 큰 경우\n",
        "      self._step += 1                 # step을 1 증가\n",
        "      if self._step > self.patience:  # step이 임계점을 넘은 경우\n",
        "        if self.verbose:\n",
        "          print(f'Early Stopping')\n",
        "        return True                   # 학습을 종료\n",
        "    else:                             # 현재의 loss가 기존 loss보다 큰 경우\n",
        "      self._step = 0                  # step을 초기화\n",
        "      self._loss = loss               # 기존 loss를 현재의 loss로 변경함\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rTJhwExdkiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = EarlyStopping(patience=10, verbose=1)  # patience를 10으로 초기화 (10번동안 loss가 나아지지 않으면 학습 종료)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJas6VsSbfUn",
        "colab_type": "code",
        "outputId": "a6aa5f44-d101-469b-f90e-28bdda81c6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "for step in range(300):\n",
        "    sess.run(train_step, feed_dict={x: train_images, y_: train_labels})\n",
        "    val_loss = sess.run(loss, feed_dict={x: train_images, y_: train_labels})\n",
        "    print(step, \" : \" , val_loss)\n",
        "    if early_stopping.validate(val_loss):\n",
        "      break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  :  1.9266083\n",
            "1  :  1.7084337\n",
            "2  :  1.5755286\n",
            "3  :  1.6059499\n",
            "4  :  1.3999399\n",
            "5  :  1.4226549\n",
            "6  :  1.2917721\n",
            "7  :  1.275231\n",
            "8  :  1.2150471\n",
            "9  :  1.204921\n",
            "10  :  1.1770998\n",
            "11  :  1.1087422\n",
            "12  :  1.099526\n",
            "13  :  1.1110641\n",
            "14  :  1.0563931\n",
            "15  :  1.1102821\n",
            "16  :  1.0461823\n",
            "17  :  1.1444799\n",
            "18  :  1.0284288\n",
            "19  :  1.0827148\n",
            "20  :  1.031743\n",
            "21  :  1.1090777\n",
            "22  :  0.96535844\n",
            "23  :  1.0236986\n",
            "24  :  0.97868973\n",
            "25  :  1.068339\n",
            "26  :  0.91454357\n",
            "27  :  0.97292536\n",
            "28  :  0.9365897\n",
            "29  :  1.0262529\n",
            "30  :  0.88190454\n",
            "31  :  0.93996215\n",
            "32  :  0.9030395\n",
            "33  :  0.98026466\n",
            "34  :  0.863059\n",
            "35  :  0.9211901\n",
            "36  :  0.8749036\n",
            "37  :  0.93588084\n",
            "38  :  0.85099924\n",
            "39  :  0.9047046\n",
            "40  :  0.8520448\n",
            "41  :  0.90045923\n",
            "42  :  0.8393766\n",
            "43  :  0.88460654\n",
            "44  :  0.83442694\n",
            "45  :  0.8744009\n",
            "46  :  0.82660174\n",
            "47  :  0.8633693\n",
            "48  :  0.82033294\n",
            "49  :  0.8532508\n",
            "50  :  0.81395847\n",
            "51  :  0.843638\n",
            "52  :  0.80795443\n",
            "53  :  0.834472\n",
            "54  :  0.8021613\n",
            "55  :  0.82574075\n",
            "56  :  0.7965891\n",
            "57  :  0.817392\n",
            "58  :  0.7912135\n",
            "59  :  0.80939984\n",
            "60  :  0.78602105\n",
            "61  :  0.80173707\n",
            "62  :  0.78099805\n",
            "63  :  0.79438114\n",
            "64  :  0.77613336\n",
            "65  :  0.7873121\n",
            "66  :  0.7714168\n",
            "67  :  0.78051275\n",
            "68  :  0.76683944\n",
            "69  :  0.7739673\n",
            "70  :  0.7623935\n",
            "71  :  0.7676615\n",
            "72  :  0.7580717\n",
            "73  :  0.76158243\n",
            "74  :  0.7538677\n",
            "75  :  0.75571835\n",
            "76  :  0.7497756\n",
            "77  :  0.75005805\n",
            "78  :  0.74578995\n",
            "79  :  0.7445915\n",
            "80  :  0.741906\n",
            "81  :  0.7393089\n",
            "82  :  0.7381191\n",
            "83  :  0.7342014\n",
            "84  :  0.734425\n",
            "85  :  0.72926056\n",
            "86  :  0.73082\n",
            "87  :  0.72447824\n",
            "88  :  0.72730017\n",
            "89  :  0.7198469\n",
            "90  :  0.72386223\n",
            "91  :  0.7153594\n",
            "92  :  0.7205028\n",
            "93  :  0.7110091\n",
            "94  :  0.717219\n",
            "95  :  0.7067893\n",
            "96  :  0.7140078\n",
            "97  :  0.702694\n",
            "98  :  0.7108664\n",
            "99  :  0.6987175\n",
            "100  :  0.70779234\n",
            "101  :  0.69485414\n",
            "102  :  0.7047831\n",
            "103  :  0.6910988\n",
            "104  :  0.70183635\n",
            "105  :  0.6874464\n",
            "106  :  0.6989497\n",
            "107  :  0.6838924\n",
            "108  :  0.6961212\n",
            "109  :  0.68043244\n",
            "110  :  0.6933486\n",
            "111  :  0.677062\n",
            "112  :  0.6906301\n",
            "113  :  0.67377734\n",
            "114  :  0.6879639\n",
            "115  :  0.6705746\n",
            "116  :  0.68534803\n",
            "117  :  0.6674503\n",
            "118  :  0.682781\n",
            "119  :  0.66440105\n",
            "120  :  0.6802611\n",
            "121  :  0.66142356\n",
            "122  :  0.6777867\n",
            "123  :  0.65851486\n",
            "124  :  0.6753566\n",
            "125  :  0.65567213\n",
            "126  :  0.67296916\n",
            "127  :  0.6528926\n",
            "128  :  0.6706231\n",
            "129  :  0.6501737\n",
            "130  :  0.6683172\n",
            "131  :  0.64751303\n",
            "132  :  0.66605014\n",
            "133  :  0.6449083\n",
            "134  :  0.663821\n",
            "135  :  0.64235723\n",
            "136  :  0.66162825\n",
            "137  :  0.63985795\n",
            "138  :  0.65947115\n",
            "139  :  0.6374083\n",
            "140  :  0.6573486\n",
            "141  :  0.6350065\n",
            "142  :  0.65525955\n",
            "143  :  0.6326508\n",
            "144  :  0.6532032\n",
            "145  :  0.6303393\n",
            "146  :  0.6511783\n",
            "147  :  0.6280706\n",
            "148  :  0.6491845\n",
            "149  :  0.625843\n",
            "150  :  0.6472206\n",
            "151  :  0.6236551\n",
            "152  :  0.64528596\n",
            "153  :  0.62150526\n",
            "154  :  0.6433798\n",
            "155  :  0.6193924\n",
            "156  :  0.6415013\n",
            "157  :  0.6173148\n",
            "158  :  0.63964975\n",
            "159  :  0.6152715\n",
            "160  :  0.6378245\n",
            "161  :  0.6132609\n",
            "162  :  0.63602495\n",
            "163  :  0.6112821\n",
            "164  :  0.6342503\n",
            "165  :  0.6093336\n",
            "166  :  0.63249993\n",
            "167  :  0.6074143\n",
            "168  :  0.63077337\n",
            "169  :  0.60552293\n",
            "170  :  0.62906986\n",
            "171  :  0.6036583\n",
            "172  :  0.62738895\n",
            "173  :  0.6018193\n",
            "174  :  0.6257299\n",
            "175  :  0.6000047\n",
            "176  :  0.62409234\n",
            "177  :  0.59821326\n",
            "178  :  0.6224756\n",
            "179  :  0.5964439\n",
            "180  :  0.62087935\n",
            "181  :  0.5946953\n",
            "182  :  0.61930287\n",
            "183  :  0.5929663\n",
            "184  :  0.6177458\n",
            "185  :  0.5912557\n",
            "186  :  0.61620754\n",
            "187  :  0.5895624\n",
            "188  :  0.6146878\n",
            "189  :  0.5878852\n",
            "190  :  0.61318624\n",
            "191  :  0.5862232\n",
            "192  :  0.6117025\n",
            "193  :  0.5845755\n",
            "194  :  0.61023647\n",
            "195  :  0.5829414\n",
            "196  :  0.6087879\n",
            "197  :  0.581321\n",
            "198  :  0.60735714\n",
            "199  :  0.57971495\n",
            "200  :  0.60594475\n",
            "201  :  0.5781247\n",
            "202  :  0.6045515\n",
            "203  :  0.5765535\n",
            "204  :  0.60317886\n",
            "205  :  0.575007\n",
            "206  :  0.6018293\n",
            "207  :  0.57349426\n",
            "208  :  0.60050625\n",
            "209  :  0.57202905\n",
            "210  :  0.59921473\n",
            "211  :  0.5706324\n",
            "212  :  0.5979618\n",
            "213  :  0.56933504\n",
            "214  :  0.5967567\n",
            "215  :  0.56818\n",
            "216  :  0.5956111\n",
            "217  :  0.5672263\n",
            "218  :  0.59453815\n",
            "219  :  0.5665488\n",
            "220  :  0.59355\n",
            "221  :  0.5662349\n",
            "222  :  0.5926534\n",
            "223  :  0.5663698\n",
            "224  :  0.5918406\n",
            "225  :  0.5670094\n",
            "226  :  0.59108144\n",
            "227  :  0.56813806\n",
            "228  :  0.5903171\n",
            "229  :  0.56962836\n",
            "230  :  0.589466\n",
            "231  :  0.5712311\n",
            "232  :  0.5884471\n",
            "Early Stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MMBv7yjUulo",
        "colab_type": "code",
        "outputId": "a12c7de5-4190-4f6b-8438-d79f42cc8094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(\"Accuracy : %.4f\" % sess.run(accuracy, feed_dict={x: test_images, y_: test_labels}))\n",
        "# 위에서 설명드린 것처럼 learning_rate를 보다 줄이고, epoch을 보다 늘렸을 때 accuracy가 원래의 모델보다 좋게 나왔습니다."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.7921\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}